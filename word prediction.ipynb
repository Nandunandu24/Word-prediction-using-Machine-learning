{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNcvBVbOT94pvaI0usb4qtW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"source":["from google.colab import drive\n","import zipfile\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Path to the zip file in your Google Drive\n","zip_file_path = '/content/drive/MyDrive/book.zip'  # Update with your file path\n","\n","# Path to extract the file to\n","extract_path = '/content/'  # Extract to the current working directory\n","\n","# Extract the file\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    zip_ref.extract('sherlock-holm.es_stories_plain-text_advs.txt', extract_path)\n","\n","print(f\"File extracted to: {extract_path}sherlock-holm.es_stories_plain-text_advs.txt\")"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6SMpWOs6Pyy2","executionInfo":{"status":"ok","timestamp":1739029717626,"user_tz":-330,"elapsed":26899,"user":{"displayName":"Nandini R","userId":"01595352391549947437"}},"outputId":"35c28b37-310b-4708-d21d-449dfe05171e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","File extracted to: /content/sherlock-holm.es_stories_plain-text_advs.txt\n"]}]},{"source":["!pip install tensorflow\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from google.colab import drive\n","import zipfile\n","\n","# Mount Google Drive to access the text file\n","drive.mount('/content/drive')\n","\n","# Path to the zip file in your Google Drive\n","zip_file_path = '/content/drive/MyDrive/book.zip'\n","\n","# Path to extract the file to\n","extract_path = '/content/'\n","\n","# Extract the file\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    zip_ref.extract('sherlock-holm.es_stories_plain-text_advs.txt', extract_path)\n","\n","print(f\"File extracted to: {extract_path}sherlock-holm.es_stories_plain-text_advs.txt\")\n","\n","# Read the text file after it is extracted\n","with open('sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n","    text = file.read()\n","\n","# Now you can use the 'text' variable\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","total_words = len(tokenizer.word_index) + 1\n","\n","print(f\"Total words: {total_words}\")"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9XxYNwVRQJDH","executionInfo":{"status":"ok","timestamp":1739029789196,"user_tz":-330,"elapsed":7757,"user":{"displayName":"Nandini R","userId":"01595352391549947437"}},"outputId":"26f6031d-e962-43da-ce94-72e702a10419"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","File extracted to: /content/sherlock-holm.es_stories_plain-text_advs.txt\n","Total words: 8200\n"]}]},{"cell_type":"code","source":["input_sequences = []\n","for line in text.split('\\n'):\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequences.append(n_gram_sequence)"],"metadata":{"id":"OIF7gZgKQUTm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_sequence_len = max([len(seq) for seq in input_sequences])\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"],"metadata":{"id":"tJempqA8QY41"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = input_sequences[:, :-1]\n","y = input_sequences[:, -1]"],"metadata":{"id":"soJDz4ILQdmK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"],"metadata":{"id":"lRoqSXfJQgqA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Sequential()\n","model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n","model.add(LSTM(150))\n","model.add(Dense(total_words, activation='softmax'))\n","print(model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"Ip9CVL93QlTv","executionInfo":{"status":"ok","timestamp":1739029897883,"user_tz":-330,"elapsed":200,"user":{"displayName":"Nandini R","userId":"01595352391549947437"}},"outputId":"3cc83127-0c0b-44c4-ee83-1424b8c98c28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["None\n"]}]},{"source":["# Previous code (likely in a different cell)\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from google.colab import drive  # Import drive\n","import zipfile\n","\n","# ... (Code to load and preprocess text data)\n","\n","# 1. Mount Google Drive to access the text file\n","drive.mount('/content/drive')\n","\n","# 2. Path to the zip file in your Google Drive\n","zip_file_path = '/content/drive/MyDrive/book.zip'\n","\n","# 3. Path to extract the file to\n","extract_path = '/content/'\n","\n","# 4. Extract the file\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    zip_ref.extract('sherlock-holm.es_stories_plain-text_advs.txt', extract_path)\n","\n","# 5. Read the text file after it is extracted\n","with open('sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n","    text = file.read()\n","\n","# Now you can use the 'text' variable\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","total_words = len(tokenizer.word_index) + 1  # Define total_words here\n","\n","# ... (Rest of your code to create input sequences)\n","\n","# Define max_sequence_len\n","max_sequence_len = max([len(seq) for seq in input_sequences])\n","\n","# ... (Rest of your code to create X and y)\n","\n","# Define"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9w8sxEDFQ-7h","executionInfo":{"status":"ok","timestamp":1739030185885,"user_tz":-330,"elapsed":3560,"user":{"displayName":"Nandini R","userId":"01595352391549947437"}},"outputId":"ba08d5c7-2c19-4ac3-a508-01666f0fbbfa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"source":["# Previous code (likely in a different cell)\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from google.colab import drive  # Import drive\n","import zipfile\n","\n","# ... (Code to load and preprocess text data)\n","\n","# 1. Mount Google Drive to access the text file\n","drive.mount('/content/drive')\n","\n","# 2. Path to the zip file in your Google Drive\n","zip_file_path = '/content/drive/MyDrive/book.zip'\n","\n","# 3. Path to extract the file to\n","extract_path = '/content/'\n","\n","# 4. Extract the file\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    zip_ref.extract('sherlock-holm.es_stories_plain-text_advs.txt', extract_path)\n","\n","# 5. Read the text file after it is extracted\n","with open('sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n","    text = file.read()\n","\n","# Now you can use the 'text' variable\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","total_words = len(tokenizer.word_index) + 1  # Define total_words here\n","\n","# Create input sequences here before calculating max_sequence_len\n","input_sequences = []\n","for line in text.split('\\n'):\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequences.append(n_gram_sequence)\n","\n","# Define max_sequence_len\n","max_sequence_len = max([len(seq) for seq in input_sequences])\n","\n","# ... (Rest of your code to create X and y)\n","\n","# Define"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0leUVl9nRILE","executionInfo":{"status":"ok","timestamp":1739030043528,"user_tz":-330,"elapsed":3942,"user":{"displayName":"Nandini R","userId":"01595352391549947437"}},"outputId":"2960d002-86a9-4a78-c90f-aefa6c3f1810"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_sKVjZMgSStK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed_text = \"I will leave if they\"\n","next_words = 3\n","\n","for _ in range(next_words):\n","    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","    predicted = np.argmax(model.predict(token_list), axis=-1)\n","    output_word = \"\"\n","    for word, index in tokenizer.word_index.items():\n","        if index == predicted:\n","            output_word = word\n","            break\n","    seed_text += \" \" + output_word\n","\n","print(seed_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"635Ui20SRS8y","executionInfo":{"status":"ok","timestamp":1739030170966,"user_tz":-330,"elapsed":1267,"user":{"displayName":"Nandini R","userId":"01595352391549947437"}},"outputId":"22fd8119-94da-4777-a0cb-94afb438e0e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 966ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","I will leave if they butler worked worked\n"]}]},{"source":["# Previous code (likely in a different cell)\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from google.colab import drive  # Import drive\n","import zipfile\n","\n","# ... (Code to load and preprocess text data)\n","\n","# 1. Mount Google Drive to access the text file\n","drive.mount('/content/drive')\n","\n","# 2. Path to the zip file in your Google Drive\n","zip_file_path = '/content/drive/MyDrive/book.zip'\n","\n","# 3. Path to extract the file to\n","extract_path = '/content/'\n","\n","# 4. Extract the file\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    zip_ref.extract('sherlock-holm.es_stories_plain-text_advs.txt', extract_path)\n","\n","# 5. Read the text file after it is extracted\n","with open('sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n","    text = file.read()\n","\n","# Now you can use the 'text' variable\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","total_words = len(tokenizer.word_index) + 1  # Define total_words here\n","\n","# Create input sequences here before calculating max_sequence_len\n","input_sequences = []\n","for line in text.split('\\n'):\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequences.append(n_gram_sequence)\n","\n","# Define max_sequence_len\n","max_sequence_len = max([len(seq) for seq in input_sequences])\n","\n","# ... (Rest of your code to create X and y)\n","\n","# Define your model here and make sure this cell is executed before the prediction cell\n","model = Sequential()\n","model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n","model.add(LSTM(150))\n","model.add(Dense(total_words, activation='softmax'))\n","print(model.summary())"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"id":"00GvHtOSRe0i","executionInfo":{"status":"ok","timestamp":1739030138020,"user_tz":-330,"elapsed":5213,"user":{"displayName":"Nandini R","userId":"01595352391549947437"}},"outputId":"24bbfdc9-d5c8-43d3-98d6-5a86c32c46bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["None\n"]}]},{"cell_type":"code","source":["seed_text = \"I will leave if they marry\"\n","next_words = 3\n","\n","for _ in range(next_words):\n","    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","    predicted = np.argmax(model.predict(token_list), axis=-1)\n","    output_word = \"\"\n","    for word, index in tokenizer.word_index.items():\n","        if index == predicted:\n","            output_word = word\n","            break\n","    seed_text += \" \" + output_word\n","\n","print(seed_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8P9yZVWSWyQ","executionInfo":{"status":"ok","timestamp":1739030382272,"user_tz":-330,"elapsed":282,"user":{"displayName":"Nandini R","userId":"01595352391549947437"}},"outputId":"57d90d0e-30d6-4f75-cd0a-51fd5ae0dc08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","I will leave if they marry complained complained shoves\n"]}]}]}